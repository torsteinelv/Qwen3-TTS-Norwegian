#!/usr/bin/env python3
"""
Qwen3-TTS Norwegian Fine-Tuning (WORKING Sub-Talker + Language)
==============================================================
Fixer de to vanligste √•rsakene til:
  - "st√∏y/skurring" (manglende sub-talker loss)
  - "helt lik base" (LoRA brukes, text_projection trenes, korrekt lagring)

VIKTIG:
- Qwen3TTSTalkerOutputWithPast.hidden_states er en tuple:
    (decoder_hidden_states, codec_ids)
  Derfor m√• vi hente decoder hidden states via hidden_states[0].

Kj√∏ring:
  accelerate launch src/train_norwegian_fixed.py \
    --train_jsonl /workspace/data/train_with_codes.jsonl \
    --init_model_path /workspace/base_model \
    --output_model_path /workspace/output/run_norwegian \
    --batch_size 4 \
    --num_epochs 100 \
    --lr 1e-5 \
    --sub_loss_weight 0.25 \
    --save_every 1 \
    --hf_repo_id telvenes/qwen3-tts-norsk-finetune
"""

import argparse
import json
import os
import sys
from typing import List, Tuple, Union

import librosa
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW

from accelerate import Accelerator
from transformers import AutoConfig
from huggingface_hub import HfApi

from peft import LoraConfig, get_peft_model
try:
    from peft import TaskType
except Exception:
    TaskType = None  # fallback

# =========================
# 0) IMPORT QWEN3-TTS
# =========================
sys.path.append("/workspace/Qwen3-TTS")
try:
    from qwen_tts.inference.qwen3_tts_model import Qwen3TTSModel
    from qwen_tts.core.models.modeling_qwen3_tts import mel_spectrogram
except ImportError:
    print("‚ö†Ô∏è Kunne ikke importere Qwen3TTSModel. Sjekk at /workspace/Qwen3-TTS finnes.")
    raise


AudioLike = Union[str, np.ndarray, Tuple[np.ndarray, int]]

# =========================
# 1) DATASET
# =========================
class NorwegianTTSDataset(Dataset):
    def __init__(self, jsonl_path: str, processor, config, max_audio_sec: float = 15.0):
        self.processor = processor
        self.config = config
        self.max_audio_samples = int(24000 * max_audio_sec)

        with open(jsonl_path, "r", encoding="utf-8") as f:
            self.items = [json.loads(line) for line in f]
        print(f"‚úÖ Lastet {len(self.items)} eksempler.")

    def __len__(self):
        return len(self.items)

    def _load_audio_24k(self, path: str) -> Tuple[np.ndarray, int]:
        try:
            audio, sr = librosa.load(path, sr=24000, mono=True)  # tving 24k
            if audio.shape[0] > self.max_audio_samples:
                audio = audio[: self.max_audio_samples]
            return audio.astype(np.float32), int(sr)
        except Exception as e:
            print(f"‚ö†Ô∏è Audio load error ({path}): {e}")
            return np.zeros(24000, dtype=np.float32), 24000

    def _build_assistant_text(self, text: str) -> str:
        # samme format som du har brukt
        return f"<|im_start|>assistant\n{text}<|im_end|>\n<|im_start|>assistant\n"

    def _tokenize_text(self, text: str) -> torch.Tensor:
        out = self.processor(text=text, return_tensors="pt", padding=True)
        ids = out["input_ids"]
        if ids.dim() == 1:
            ids = ids.unsqueeze(0)
        return ids

    @torch.inference_mode()
    def extract_mels(self, audio: np.ndarray) -> torch.Tensor:
        mels = mel_spectrogram(
            torch.from_numpy(audio).unsqueeze(0),
            n_fft=1024,
            num_mels=128,
            sampling_rate=24000,
            hop_size=256,
            win_size=1024,
            fmin=0,
            fmax=12000,
        ).transpose(1, 2)  # (1, T, 128)
        return mels

    def __getitem__(self, idx: int):
        item = self.items[idx]

        ref_path = item.get("ref_audio") or item.get("ref_audio_path") or item.get("audio_path")
        if not ref_path:
            # fallback til neste
            return self.__getitem__((idx + 1) % len(self.items))

        text = self._build_assistant_text(item["text"])
        text_ids = self._tokenize_text(text)

        audio_codes = torch.tensor(item["audio_codes"], dtype=torch.long)  # (N, 16)

        wav, _sr = self._load_audio_24k(ref_path)
        ref_mel = self.extract_mels(wav)

        # du har brukt text_ids[:,:-5] f√∏r ‚Äì beholdt for kompat.
        return {
            "text_ids": text_ids[:, :-5],
            "audio_codes": audio_codes,
            "ref_mel": ref_mel,
        }

    def collate_fn(self, batch):
        item_length = [b["text_ids"].shape[1] + b["audio_codes"].shape[0] for b in batch]
        max_length = max(item_length) + 8
        B, T = len(batch), max_length

        input_ids = torch.zeros((B, T, 2), dtype=torch.long)
        codec_ids = torch.zeros((B, T, 16), dtype=torch.long)

        text_embedding_mask = torch.zeros((B, T), dtype=torch.bool)
        codec_embedding_mask = torch.zeros((B, T), dtype=torch.bool)
        codec_mask = torch.zeros((B, T), dtype=torch.bool)
        attention_mask = torch.zeros((B, T), dtype=torch.long)

        codec_0_labels = torch.full((B, T), -100, dtype=torch.long)

        for i, data in enumerate(batch):
            text_ids = data["text_ids"]
            audio_codes = data["audio_codes"]  # (N,16)
            audio_codec_0 = audio_codes[:, 0]  # (N,)

            text_len = text_ids.shape[1]
            codec_len = audio_codec_0.shape[0]

            # TEXT CHANNEL (0)
            input_ids[i, :3, 0] = text_ids[0, :3]
            input_ids[i, 3:7, 0] = self.config.tts_pad_token_id
            input_ids[i, 7, 0] = self.config.tts_bos_token_id
            input_ids[i, 8 : 8 + text_len - 3, 0] = text_ids[0, 3:]
            input_ids[i, 8 + text_len - 3, 0] = self.config.tts_eos_token_id
            input_ids[i, 8 + text_len - 2 : 8 + text_len + codec_len, 0] = self.config.tts_pad_token_id

            text_embedding_mask[i, : 8 + text_len + codec_len] = True

            # CODEC CHANNEL (1)
            input_ids[i, 3:8, 1] = torch.tensor(
                [
                    self.config.talker_config.codec_nothink_id,
                    self.config.talker_config.codec_think_bos_id,
                    self.config.talker_config.codec_think_eos_id,
                    0,
                    self.config.talker_config.codec_pad_id,
                ],
                dtype=torch.long,
            )

            input_ids[i, 8 : 8 + text_len - 3, 1] = self.config.talker_config.codec_pad_id
            input_ids[i, 8 + text_len - 3, 1] = self.config.talker_config.codec_pad_id
            input_ids[i, 8 + text_len - 2, 1] = self.config.talker_config.codec_bos_id

            # selve codec_0 token-str√∏mmen
            start = 8 + text_len - 1
            input_ids[i, start : start + codec_len, 1] = audio_codec_0
            input_ids[i, start + codec_len, 1] = self.config.talker_config.codec_eos_token_id

            # labels (codec_0)
            codec_0_labels[i, start : start + codec_len] = audio_codec_0
            codec_0_labels[i, start + codec_len] = self.config.talker_config.codec_eos_token_id

            # full 16-code groups alignet p√• codec-posisjoner
            codec_ids[i, start : start + codec_len, :] = audio_codes

            # masks
            codec_embedding_mask[i, 3 : 8 + text_len + codec_len] = True
            codec_embedding_mask[i, 6] = False  # speaker slot

            codec_mask[i, start : start + codec_len] = True
            attention_mask[i, : 8 + text_len + codec_len] = 1

        ref_mels = torch.cat([b["ref_mel"] for b in batch], dim=0)

        return {
            "input_ids": input_ids,
            "codec_ids": codec_ids,
            "codec_0_labels": codec_0_labels,
            "codec_mask": codec_mask,
            "attention_mask": attention_mask,
            "text_embedding_mask": text_embedding_mask.unsqueeze(-1),
            "codec_embedding_mask": codec_embedding_mask.unsqueeze(-1),
            "ref_mels": ref_mels,
        }


# =========================
# 2) HELPERS
# =========================
def extract_last_hidden(outputs) -> torch.Tensor | None:
    """
    Qwen3TTSTalkerOutputWithPast.hidden_states = (decoder_hidden_states, codec_ids)
    decoder_hidden_states er en tuple med (layer0, layer1, ..., last)
    """
    hs = getattr(outputs, "hidden_states", None)
    if hs is None:
        return None

    # forventet: (decoder_hidden_states, codec_ids)
    if isinstance(hs, (tuple, list)) and len(hs) == 2:
        decoder_hs = hs[0]
    else:
        decoder_hs = hs

    if decoder_hs is None:
        return None

    # noen ganger kan det v√¶re en tuple/list av tensors
    if isinstance(decoder_hs, (tuple, list)) and len(decoder_hs) > 0 and torch.is_tensor(decoder_hs[-1]):
        return decoder_hs[-1]

    # fallback hvis det faktisk er en tensor
    if torch.is_tensor(decoder_hs):
        return decoder_hs

    return None


# =========================
# 3) TRAIN
# =========================
def train():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_jsonl", type=str, required=True)
    parser.add_argument("--init_model_path", type=str, required=True)
    parser.add_argument("--output_model_path", type=str, default="./qwen3-tts-norwegian-lora")
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--save_every", type=int, default=1)
    parser.add_argument("--sub_loss_weight", type=float, default=0.25)  # viktig
    parser.add_argument("--max_audio_sec", type=float, default=15.0)
    parser.add_argument("--hf_repo_id", type=str, default=os.getenv("HF_REPO_ID"))
    args = parser.parse_args()

    accelerator = Accelerator(
        gradient_accumulation_steps=4,
        mixed_precision="bf16",
        log_with="tensorboard",
        project_dir=args.output_model_path,
    )

    if accelerator.is_main_process:
        os.makedirs(args.output_model_path, exist_ok=True)
        print("üöÄ Starter norsk TTS-trening (WORKING Sub-Talker)")
        print(f"   LR: {args.lr} | sub_loss_weight={args.sub_loss_weight}")
        if args.hf_repo_id:
            print(f"‚òÅÔ∏è  HF Upload: {args.hf_repo_id}")

    hf_api = HfApi() if (args.hf_repo_id and accelerator.is_main_process) else None

    qwen_wrapper = Qwen3TTSModel.from_pretrained(
        args.init_model_path,
        dtype=torch.bfloat16,
        device_map={"": accelerator.device},
        attn_implementation="flash_attention_2" if torch.cuda.is_available() else None,
    )
    model = qwen_wrapper.model
    model.requires_grad_(False)

    # LoRA p√• talker.model (hoved-LM)
    peft_kwargs = dict(
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )
    if TaskType is not None:
        peft_kwargs["task_type"] = TaskType.CAUSAL_LM
    else:
        peft_kwargs["task_type"] = "CAUSAL_LM"

    peft_config = LoraConfig(**peft_kwargs)
    model.talker.model = get_peft_model(model.talker.model, peft_config)

    # Unfreeze text_projection (spr√•k/uttale-siden)
    if hasattr(model.talker, "text_projection"):
        for p in model.talker.text_projection.parameters():
            p.requires_grad = True

    if accelerator.is_main_process:
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"‚úÖ Trainable params: {trainable:,}")

    # Load config
    try:
        config = AutoConfig.from_pretrained(args.init_model_path)
    except Exception:
        config = AutoConfig.from_pretrained("Qwen/Qwen3-TTS-12Hz-1.7B-Base")

    dataset = NorwegianTTSDataset(args.train_jsonl, qwen_wrapper.processor, config, max_audio_sec=args.max_audio_sec)
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=dataset.collate_fn,
        num_workers=2,
        pin_memory=True,
    )

    optimizer = AdamW(
        (p for p in model.parameters() if p.requires_grad),
        lr=args.lr,
        weight_decay=0.01,
    )

    model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)

    warned_once = False
    model.train()

    for epoch in range(args.num_epochs):
        epoch_loss = 0.0
        n_steps = 0

        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(model):
                input_ids = batch["input_ids"].to(model.device)
                codec_ids = batch["codec_ids"].to(model.device)
                ref_mels = batch["ref_mels"].to(model.device, dtype=model.dtype)

                text_embedding_mask = batch["text_embedding_mask"].to(model.device)
                codec_embedding_mask = batch["codec_embedding_mask"].to(model.device)
                attention_mask = batch["attention_mask"].to(model.device)
                codec_0_labels = batch["codec_0_labels"].to(model.device)
                codec_mask = batch["codec_mask"].to(model.device)

                # Speaker embedding
                speaker_embedding = model.speaker_encoder(ref_mels).detach()

                input_text_ids = input_ids[:, :, 0]
                input_codec_ids = input_ids[:, :, 1]

                # Text embeds
                raw_text = model.talker.model.text_embedding(input_text_ids)
                proj_text = model.talker.text_projection(raw_text)
                text_emb = proj_text * text_embedding_mask

                # Codec embeds + speaker injection
                codec_emb = model.talker.model.codec_embedding(input_codec_ids) * codec_embedding_mask
                codec_emb[:, 6, :] = speaker_embedding

                inputs_embeds = text_emb + codec_emb

                # Main loss: codec_0
                outputs = model.talker(
                    inputs_embeds=inputs_embeds[:, :-1, :],
                    attention_mask=attention_mask[:, :-1],
                    labels=codec_0_labels[:, 1:],       # shift
                    output_hidden_states=True,
                )

                loss_main = outputs.loss
                loss = loss_main

                # Sub-talker loss (code groups 2-16) ‚Äì KORREKT hidden-state extraction + shift
                last_hidden = extract_last_hidden(outputs)  # (B, L, H) der L = T-1

                if last_hidden is None:
                    if (not warned_once) and accelerator.is_main_process:
                        print("‚ö†Ô∏è Hidden states ikke tilgjengelig -> sub-talker loss hoppes over (skal normalt ikke skje).")
                        warned_once = True
                else:
                    # Align: outputs length L = T-1, labels er [1:], s√• vi bruker mask/codes [1:]
                    active_mask = codec_mask[:, 1:]                 # (B, L)
                    if active_mask.any():
                        hs = last_hidden[active_mask]              # (N, H)
                        codes = codec_ids[:, 1:, :][active_mask]   # (N, 16)

                        _, sub_loss = model.talker.forward_sub_talker_finetune(
                            codec_ids=codes,
                            talker_hidden_states=hs,
                        )
                        loss = loss_main + args.sub_loss_weight * sub_loss

                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()

                epoch_loss += float(loss.detach().item())
                n_steps += 1

        avg_loss = epoch_loss / max(1, n_steps)
        if accelerator.is_main_process:
            print(f"üìä Epoch {epoch+1}/{args.num_epochs} | Loss: {avg_loss:.4f}")

            if (epoch + 1) % args.save_every == 0:
                save_path = os.path.join(args.output_model_path, f"epoch-{epoch+1}")
                os.makedirs(save_path, exist_ok=True)

                unwrapped = accelerator.unwrap_model(model)

                # Lagre LoRA adapter (talker.model)
                unwrapped.talker.model.save_pretrained(save_path)

                # Lagre text_projection
                if hasattr(unwrapped.talker, "text_projection"):
                    torch.save(
                        unwrapped.talker.text_projection.state_dict(),
                        os.path.join(save_path, "text_projection.bin"),
                    )

                # liten readme
                with open(os.path.join(save_path, "README.md"), "w", encoding="utf-8") as f:
                    f.write(
                        "\n".join(
                            [
                                "---",
                                "base_model: Qwen/Qwen3-TTS-12Hz-1.7B-Base",
                                "library_name: peft",
                                f"tags: [text-to-speech, norwegian, lora, epoch-{epoch+1}]",
                                "---",
                                f"# Qwen3-TTS Norwegian - Epoch {epoch+1}",
                                "",
                                f"- lr: {args.lr}",
                                f"- sub_loss_weight: {args.sub_loss_weight}",
                                "- includes: sub-talker loss + text_projection training",
                            ]
                        )
                    )

                print(f"üíæ Lagret: {save_path}")

                if hf_api:
                    try:
                        hf_api.upload_folder(
                            folder_path=save_path,
                            repo_id=args.hf_repo_id,
                            path_in_repo=f"checkpoints/epoch_{epoch+1}",
                            repo_type="model",
                        )
                        print("‚òÅÔ∏è HF Upload OK")
                    except Exception as e:
                        print(f"‚ö†Ô∏è HF Upload Feil: {e}")


if __name__ == "__main__":
    train()
